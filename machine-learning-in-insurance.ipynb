{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10687264,"sourceType":"datasetVersion","datasetId":6621489},{"sourceId":10687835,"sourceType":"datasetVersion","datasetId":6621925}],"dockerImageVersionId":30839,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"                                        # ML in Insurance\n\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Wed Feb  5 14:10:14 2025\n\n@author: cheikh\n\"\"\"\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\n\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-07T13:26:44.820570Z","iopub.execute_input":"2025-02-07T13:26:44.820931Z","iopub.status.idle":"2025-02-07T13:26:44.826909Z","shell.execute_reply.started":"2025-02-07T13:26:44.820900Z","shell.execute_reply":"2025-02-07T13:26:44.825595Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"# Chargement des données\ndata = pd.read_csv('/kaggle/input/kaggle-database-2/TravelInsurancePrediction.csv')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T13:37:34.582511Z","iopub.execute_input":"2025-02-07T13:37:34.582850Z","iopub.status.idle":"2025-02-07T13:37:34.591552Z","shell.execute_reply.started":"2025-02-07T13:37:34.582821Z","shell.execute_reply":"2025-02-07T13:37:34.590401Z"}},"outputs":[],"execution_count":56},{"cell_type":"code","source":"# Identification des colonnes catégorielles et numériques\ncategorical_features = ['Employment Type', 'GraduateOrNot', 'FrequentFlyer', 'EverTravelledAbroad']\nnumeric_features = ['Age', 'AnnualIncome', 'FamilyMembers', 'ChronicDiseases']\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T13:38:10.099673Z","iopub.execute_input":"2025-02-07T13:38:10.100018Z","iopub.status.idle":"2025-02-07T13:38:10.104918Z","shell.execute_reply.started":"2025-02-07T13:38:10.099992Z","shell.execute_reply":"2025-02-07T13:38:10.103321Z"}},"outputs":[],"execution_count":57},{"cell_type":"code","source":"# Prétraitement des données\nX = data.drop('TravelInsurance', axis=1)\ny = data['TravelInsurance']\n\n# Création du ColumnTransformer pour gérer à la fois les variables catégorielles et numériques\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', StandardScaler(), numeric_features),\n        ('cat', OneHotEncoder(drop='first'), categorical_features)\n    ])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T13:39:08.560415Z","iopub.execute_input":"2025-02-07T13:39:08.560782Z","iopub.status.idle":"2025-02-07T13:39:08.567187Z","shell.execute_reply.started":"2025-02-07T13:39:08.560753Z","shell.execute_reply":"2025-02-07T13:39:08.565722Z"}},"outputs":[],"execution_count":59},{"cell_type":"code","source":"# Division des données en ensembles d'entraînement et de test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Définition des pipelines pour différents modèles \nmodels = {\n    'Logistic Regression': Pipeline([\n        ('preprocessor', preprocessor),\n        ('classifier', LogisticRegression(random_state=42))\n    ]),\n    'Random Forest': Pipeline([\n        ('preprocessor', preprocessor),\n        ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n    ]),\n    'Support Vector Machine': Pipeline([\n        ('preprocessor', preprocessor),\n        ('classifier', SVC(random_state=42))\n    ]),\n    'Neural Network': Pipeline([\n        ('preprocessor', preprocessor),\n        ('classifier', MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42))\n    ]),\n    'XGBoost': Pipeline([\n        ('preprocessor', preprocessor),\n        ('classifier', XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42))\n    ])\n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T13:39:27.826602Z","iopub.execute_input":"2025-02-07T13:39:27.827034Z","iopub.status.idle":"2025-02-07T13:39:27.837313Z","shell.execute_reply.started":"2025-02-07T13:39:27.827001Z","shell.execute_reply":"2025-02-07T13:39:27.836265Z"}},"outputs":[],"execution_count":60},{"cell_type":"code","source":"# Entraînement et évaluation de chaque modèle\nfor name, model in models.items():\n    # Entraînement du modèle\n    model.fit(X_train, y_train)\n    \n    # Prédiction sur l'ensemble de test\n    y_pred = model.predict(X_test)\n    \n    # Évaluation du modèle\n    print(f\"--- {name} ---\")\n    print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n    print(classification_report(y_test, y_pred))\n    print(\"\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T13:39:41.585164Z","iopub.execute_input":"2025-02-07T13:39:41.585555Z","iopub.status.idle":"2025-02-07T13:39:46.528040Z","shell.execute_reply.started":"2025-02-07T13:39:41.585522Z","shell.execute_reply":"2025-02-07T13:39:46.527012Z"}},"outputs":[{"name":"stdout","text":"--- Logistic Regression ---\nAccuracy: 0.7688442211055276\n              precision    recall  f1-score   support\n\n           0       0.77      0.92      0.84       257\n           1       0.77      0.50      0.60       141\n\n    accuracy                           0.77       398\n   macro avg       0.77      0.71      0.72       398\nweighted avg       0.77      0.77      0.75       398\n\n\n\n--- Random Forest ---\nAccuracy: 0.8140703517587939\n              precision    recall  f1-score   support\n\n           0       0.82      0.91      0.86       257\n           1       0.80      0.64      0.71       141\n\n    accuracy                           0.81       398\n   macro avg       0.81      0.77      0.79       398\nweighted avg       0.81      0.81      0.81       398\n\n\n\n--- Support Vector Machine ---\nAccuracy: 0.8065326633165829\n              precision    recall  f1-score   support\n\n           0       0.79      0.95      0.86       257\n           1       0.86      0.54      0.66       141\n\n    accuracy                           0.81       398\n   macro avg       0.83      0.75      0.76       398\nweighted avg       0.82      0.81      0.79       398\n\n\n\n--- Neural Network ---\nAccuracy: 0.7889447236180904\n              precision    recall  f1-score   support\n\n           0       0.80      0.89      0.85       257\n           1       0.76      0.60      0.67       141\n\n    accuracy                           0.79       398\n   macro avg       0.78      0.75      0.76       398\nweighted avg       0.79      0.79      0.78       398\n\n\n\n--- XGBoost ---\nAccuracy: 0.8165829145728644\n              precision    recall  f1-score   support\n\n           0       0.82      0.92      0.87       257\n           1       0.81      0.62      0.71       141\n\n    accuracy                           0.82       398\n   macro avg       0.82      0.77      0.79       398\nweighted avg       0.82      0.82      0.81       398\n\n\n\n","output_type":"stream"}],"execution_count":61},{"cell_type":"markdown","source":"Nous avons testé cinq modèles pour la prédiction de la souscription à une assurance voyage. Le modèle de régression logistique a une accuracy de 0.7688 avec une meilleure performance pour la classe 0 (0.92 de rappel) par rapport à la classe 1 (0.50). Le Random Forest a une accuracy de 0.8141 avec un bon équilibre entre précision et rappel (0.82 et 0.91 pour la classe 0, 0.80 et 0.64 pour la classe 1). La machine à vecteurs de support (SVM) atteint une accuracy de 0.8065, avec un rappel élevé pour la classe 0 (0.95) mais faible pour la classe 1 (0.54). Le réseau neuronal a une accuracy de 0.7889, avec un rappel de 0.89 pour la classe 0 et 0.60 pour la classe 1. Enfin, XGBoost a la meilleure accuracy de 0.8166, avec des scores de précision et rappel équilibrés (0.82, 0.92 pour la classe 0 et 0.81, 0.62 pour la classe 1). Tous ces modèles montrent une tendance à mieux prédire la classe 0, indiquant un problème d'imbalance de classe où nous devons peut-être réajuster les hyperparamètres ou utiliser des techniques de gestion de l'imbalance pour améliorer les performances sur la classe 1.\n","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}}]}